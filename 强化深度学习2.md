# 深度学习
## 深度学习计算
### 层和块
nn.Sequential定义一种特殊的module,在pytorch中表示一个块的类,维护一个module组成的有序列表.
#### 自定义块
块的基本功能有将输入数据作为前向传播参数,生成输出,计算梯度,存储参数,初始化模型参数
```
class MLP(nn.Module)
    def __init__(self):
        # 调用MLP的父类Module的构造函数执行必要的初始化
        super().__init__()
        self.hiddenn = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)

    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
```

#### 顺序块
需要定义函数将块逐个追加到列表,需要将输出按追加块的顺序传递给块
```
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # module是Module子类的一个实例
            # 保存在'Module'类成员变量'_modules'中
            self._modules[str(idx)] = module

    def forward(self, X):
        for block in self._modules.values():
            X = block(X)
        return X
```

#### 在前向传播中执行代码
假设需要计算函数f(x,w) = cw^T^x的层,c是某个优化过程中未更新的常量.
```
class FixedHiddenMLP(nn.module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数
        self.rand_weight = torch.rand((20, 20), required_grad = False)
        self.Linear = nn.Linear

    def forward(self, X):
        X = self.linear(X)
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        X = self.linear(X)
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```

### 参数管理
#### 参数访问
通过Sequential定义模型时,可以通过索引访问任意层:
```
print(net[2].state_dict())
```
输出权重和偏置,为单精度浮点数float32.

#### 一次性访问所有参数
```
# 访问第一个全连接层
# named_parameters()返回迭代器,包含参数名字和本身
print(*[(name, param.shape) for name, param in net[0].named_parameters()])

# 访问全部
print(*[(name, param.shape) for name, param in net.named_parameters()])
```

### 参数初始化
#### 内置初始化
调用内置的初始化器:
```
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init_zeros_(m.bias)
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
```
也可以对不同块应用不同初始化方法:
```
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(init_xavier)
net[2].apply(init_42)
```
其它的初始化方法:
```
# 在ReLU中假设每层有一半神经元被激活
nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in')
# 正交初始化
nn.init.orthogonal(m.weight)

```

#### 自定义初始化
假设需要使用以下分布为权重参数w初始化:P(w ~ U(5,10)) = 1/4, P(w ~ U(-10,-5)) = 1/4, P(w = 0) = 1/2
```
def my_init(m):
    if type(m) == nn.Linear
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5
```

#### 延后初始化
只需要指定输出维数,输入维数未知,未初始化任何参数
```
net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(),nn.Linear(256,10))
```

### 参数绑定
希望在多个层之间共享参数, 可以定义一个层,使它的参数设置另一个参数
```
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
```

### 自定义层
#### 不带参数的层
假设要从其输入中减去其均值, 只需继承基础层类并实现前向传播:
```
class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
```

#### 带参数的层
使用内置函数nn.Parameter来创建参数:
```
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))

    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```

### 读写文件
#### 加载和保存张量
```
x = torch.arange(4)
torch.save(x, 'x-file')
x2 = torch.load('x-file')
```
#### 加载和保存模型参数
```
# 保存
torch.save(net.state_dict(), 'mlp.params')
# 加载
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()
```

## 卷积神经网络
### 多层感知机限制
输入二维图像X,隐藏表示H在数学上是一个矩阵,表示为二维张量.X~i,j~和H~i,j~表示输入图像和隐藏中(i,j)处的像素,H~i,j~ = U~i,j~ + Σ~k~Σ~l~W~i,j,k,l~X~k,l~ = U~i,j~ + Σ~a~Σ~b~V~i,j,a,b~X~i+a,j+b~, 其中W为四阶权重张量, U为偏置参数.

**平移不变性**:检测对象在X的平移,应仅导致H的平移,V,U不变,则:H~i,j~ = u + ΣΣV~a,b~X~i+a,j+b~
**局部性**:不应偏离到离(i,j)很远的地方,在a或b＞Δ时设置V=0.

#### 通道
图像包含三个通道,是长度,宽度,颜色组成的三维张量,如1024x1024x3. 隐藏H采用三维张量,其中d表示输出通道, 式子变为: H~i,j,d~ = ΣΣΣ~c~V~a,b,c,d~X~i+a,j+b,c~

### 图像卷积
卷积窗口从输入张量的左上开始,包含在窗口的部分张量与卷积核张量按元素相乘,得到张量求和得到新的输出值. 输出大小为输入大小-卷积核大小+1
```
def corr2d(X, K):
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
```
卷积层对输入和权重进行互相关运算:
```
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

#### 目标的边缘检测
通过找到像素变化位置,检测不同颜色的边缘. 首先构造6x8的黑白图像,中间四列为黑色(0),其余为白色(1), 然后构造高度为1, 宽度为2的卷积核K, 使互相关运算时水平相邻元素相同时输出为0, 否则非零
```
X = torch.ones((6, 8))
X[:, 2:6] = 0
K = tirch.tensor([[1.0, -1.0]])

Y = corr2d(X, K)
```

#### 学习卷积核
由"输入-输出"对学习由X生成Y的卷积核, 在每次迭代中比较Y与卷积层输出的平方误差, 计算梯度更新卷积核
```
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

# 二维卷积层使用四维输入和输出(批量大小, 通道, 宽度, 高度)
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # 迭代卷积核
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
```

### 填充和步幅
#### 填充
在多层卷积时常常丢失边缘像素，在输入图像边界填充0元素，设置填充卷积核大小-1的行和列，上下，左右各填充一半
```
def comp_conv2d(conv2d, X):
    # (1, 1)表示批量大小和通道数
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    # 省略前两个维度
    return Y.reshape(Y.shape[2:])

# padding = 1指每边都填充了1行1列
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding = 1)
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape
```

#### 步幅
每次滑动元素的数量称为步幅，当垂直步幅为s~h~，水平步幅为s~w~，输出形状为[(n~h~ - k~h~ + p~h~ + s~h~)/s~h~] x [(n~w~ - k~w~ + p~w~ + s~w~)/s~w~]
```
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)
```

### 多输入/输出通道
#### 多输入通道
假设输入通道数为c~i~，卷积核输入通道数也需为c~i~。可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和。
```
def corr2d_multi_in(X, K):
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
```

#### 多输出通道
对每个输出通道创建c~i~xk~h~xk~w~的卷积核，每个输出通道先获取所有输入通道，再通过卷积核计算结果
```
def corr2d_multi_in_out(X, K):
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)

K = torch.stack((K, K + 1, K + 2), 0)
```
通过stack函数增加维度

#### 1x1卷积层
1x1卷积层可以看作全连接层，以c~i~个输入值转换为c~o~个输出值，维度为c~o~xc~i~
```
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape
    c_o = K.shape[0]
    X = X.reshape((c_i, h * w))
    K = K.reshape((c_o, c_i))
    Y = torch.matmul(K, X)
    reutrn Y.reshape((c_o, h, w))
```

### 汇聚层
汇聚层能降低卷积层对位置的敏感性，同时降低对空间采样表示的敏感性。汇聚层不包含参数，通常计算汇聚窗口中所有元素的最大值或均值。
```
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i:i + p_h, j:j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()
    return Y
```
汇聚层也可以改变填充和步幅
```
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
```

### 卷积神经网络(LeNet)
LeNet由两部分组成，分别是卷积编码器（由两个卷积块组成）和全连接层密集块（由三个全连接层组成）
卷积块中的基本单元使一个卷积层，一个sigmoid激活函数和平均汇聚层
```
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernelsize=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid()
    nn.Linear(84, 10)
)

def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    timer, num_batches = d2l.Timer(), len(train_iter)

    for epoch in range(num_epochs):
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
        test_acc = evaluate_accuracy_gpu(net, test_iter)
```

## 现代卷积神经网络
### AlexNet
AlexNet由八层组成，含有五个卷积层，两个全连接隐藏层和一个全连接输出层。AlexNet使用ReLU作为激活函数。同时AlexNet在全连接层使用暂退法，增加了大量的图像增强数据。
```
net = nn.Sequential(
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),
    nn.Linear(6400, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 10)
)
```

### VGG
VGG块由一系列卷积层组成，后面加上用于空间下采样的最大汇聚层。
```
def vgg_block(num_convs, in_channels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
    return nn.Sequential(*layers)
```
对于VGG11，有五个卷积块，前两个包含一个卷积层，后三个包含两个卷积层，使用3个全连接层
```
def vgg(conv_arch):
    conv_blks = []
    in_channels = 1
    for (num_convs, out_channels) in conv_arch:
        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
        in_channels = out_channels
    return nn.Sequential(
        *conv_blks, nn.Flatten(),
        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 10)
    )
```

### 网络中的网络(NiN)
NiN在每个像素通道上分别使用多层感知机。NiN在每个像素位置应用一个全连接层（视为1x1的卷积层），即将每个像素视为单个样本，将通道维度视为不同特征。

NiN块以一个普通卷积层开始，后面是两个1x1卷积层。
```
def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),
    )
```
NiN网络使用窗口形状为11x11，5x5和3x3的卷积层，每个NiN块后有一个最大汇聚层，窗口为3x3，NiN最后输出通道数等于标签类别数，最后放一个全局平均汇聚层。
```
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0),
    nn.MaxPool2d(3, stride=2),
    nin_block(96, 256, kernel_size=5, strides=1, padding=2),
    nn.MaxPool2d(3, stride=2),
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),
    nn.MaxPool2d(3, stride=2),
    nn.Dropout(0.5),
    # 标签类别数是10
    nin_block(384, 10, kernel_size=3, strides=1, padding=1),
    nn.AdaptiveAvgPool2d((1, 1)),
    # 将四维的输出转成⼆维的输出，其形状为(批量⼤⼩,10)
    nn.Flatten()
)
```

### 含并行连结的网络(GoogLeNet)
#### Inception块
Inception块由四条并行路径组成，前三条使用窗口为1x1, 3x3和5x5的卷积层，从不同空间大小中提取信息。中间两条在输入上执行1x1卷积，以减少通道数，降低模型复杂性。第四条使用3x3最大汇聚层，再使用1x1卷积层改变通道数。

```
class Inception(nn.Module):
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        # **kwargs用于接收不定数量的参数，是一个字典。
        super(Inception, self).__init__(**kwargs)
        # 线路1
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        # 线路2
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # 线路3
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # 线路4
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        return torch.cat((p1, p2, p3, p4), dim=1)
```

#### GoogLeNet模型
使用9个Inception块和全局平均汇聚层堆叠，Inception之间的最大汇聚层降低维度。
```
# 第一模块
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
nn.ReLU(),
nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
# 第二模块
b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(),
                nn.Conv2d(64, 192, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
# 第三模块
b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                Inception(256, 128, (128, 192), (32, 96), 64),
                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
# 第四模块
b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
            Inception(512, 160, (112, 224), (24, 64), 64),
            Inception(512, 128, (128, 256), (24, 64), 64),
            Inception(512, 112, (144, 288), (32, 64), 64),
            Inception(528, 256, (160, 320), (32, 128), 128),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
# 第五模块
b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                Inception(832, 384, (192, 384), (48, 128), 128),
                nn.AdaptiveAvgPool2d((1,1)),
                nn.Flatten())

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))     
```

### 批量规范化
在每次训练迭代中，首先规范化输入，减去均值除去标准差，再应用比例系数和比例偏移。用x表示小批量B的输入，则BN(x) = γ(x - μ~B~/σ~B~) + β~B~，其中γ和β是需要学习的参数。对全连接层，输出h = H(BN(Wx + b))，H为激活函数。对于卷积层，可以在卷积层之后和激活函数之前应用批量规范化，当有多个通道时，要对每个输出执行批量规范化。
```
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    if not torch.is_grad_enabled():
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        # 判断是否是全连接层
        if len(X.shape) == 2:
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 计算通道维(axis=1)上的均值和方差
            mean = X.mean(dim = (0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 更新移动平均的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta
    return Y, moving_mean.data, moving_var.data
```
创建BatchNorm层，保存γ和β参数，在训练过程中更新，同时保存均值和方差的移动平均值。
```
class BatchNorm(nn.Module):
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # 如果X不在内存上，将moving_mean和moving_var
        # 复制到X所在显存上
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 保存更新过的moving_mean和moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
        X, self.gamma, self.beta, self.moving_mean,
        self.moving_var, eps=1e-5, momentum=0.9)
        return Y
```

#### 使用批量规范化的LeNet
```
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),
    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),
    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),
    nn.Linear(84, 10))
```
同时可以直接使用nn.BatchNorm2d和nn.BatchNorm1d，前者对应卷积层。

### 残差网络（ResNet）
#### 函数类
有特定的神经网络架构F，对于所有f∈F，存在一些参数集，可以通过训练获得。假设f*是要找到的函数，且f *不属于F，则可找到函数f~F~ *是最佳选择。为此需要设计更强大的架构F'，对于嵌套函数类，即F1包含于F2...包含于Fn，此时Fn比F1更接近f *。
残差网络的核心是每个附加层都应更容易地包含原始函数作为其元素之一。

#### 残差块
假设原始输入为x，希望学出的理想映射为f(x)，残差块需要拟合出残差映射f(x) - x，最后进行加权运算加上x。
ResNet沿用VGG的3x3卷积层设计，每个卷积层后接一个批量规范化层和ReLU激活函数，然后通过跨层数据通路跳过两个卷积运算，将输入加在最后的ReLU激活函数前。
```
class Residual(nn.Module):
    def __init__(self, input_channels, num_channels,
                use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                            kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                            kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
```
input_channels和output_channels不同时，指定1x1卷积层为true，同时stride=strides保证了X和Y的宽和高相等。

#### ResNet模型
ResNet前两层与GoogLeNet一样，ResNet每个卷积层后增加了批量规范化层。
```
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                nn.BatchNorm2d(64), nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```
ResNet使用四个残差块组成的模块，每个模块使用若干同样输出通道数的残差块，第一个模块输入，输出通道数相同。
```
def resnet_block(input_channels, num_channels, num_residuals, first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(input_channels, num_channels,
                            use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
    return blk

b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))
b3 = nn.Sequential(*resnet_block(64, 128, 2))
b4 = nn.Sequential(*resnet_block(128, 256, 2))
b5 = nn.Sequential(*resnet_block(256, 512, 2))
```
最后加入全局平均汇聚层和全连接层
```
net = nn.Sequential(b1, b2, b3, b4, b5,
                nn.AdaptiveAvgPool2d((1,1)),
                nn.Flatten(), nn.Linear(512, 10))
```

### 稠密连接网络（DenseNet）


# 强化学习
## 基础内容
### 多臂老虎机
1. ε-贪心算法
以多臂老虎机（MAB）为例，该算法每次以概率1-ε选择以往经验中期望奖励最大的拉杆，以概率ε随机选择一根拉杆。
代码实现：
```
class BernoulliBandit:
    # K表示拉杆个数
    def __init__(self, K):
        # 生成0-1随机的获奖概率
        self.probs = np.random.uniform(size=K)
        # 获奖概率最大的拉杆
        self.best_idx = np.random.uniform(size=K)
        # 最大的获奖概率
        self.best_prob = self.probs[self.best_idx]
        self.K = K

    def step(self, k):
        if np.random.rand() < self.probs[k]:
            return 1
        else:
            return 0
```
构建MAB基本框架，其中累计懊悔即为ΣQ*(a) - Q(a)
```
class Solver:
    def __init__(self, bandit):
        self.bandit = bandit
        # 每根拉杆尝试的次数
        self.counts = np.zeros(self.bandit.K)
        self.regret = 0
        # 列表记录每一步动作
        self.actions = []
        # 记录每一步累积懊悔
        self.regrets = []

    def update_regret(self, k):
        self.regret += self.bandit.best_prob - self.bandit.probs[k]
        self.regrets.append(self.regret)

    def run_one_step(self):
        raise NotImplementedError

    def run(self, num_steps):
        for _ in range(num_steps):
            k = self.run_one_step()
            self.counts[k] += 1
            self.actions.append(k)
            self.update_regret(k)
```
实现贪心算法，其继承solver类
```
class EpsilonGreedy(Solver):
    def __init__(self, bandit, epsilon=0.01, init_prob=1.0):
        super(EpsilonGreedy, self).__init__(bandit)
        self.epsilon = epsilon
        #初始化拉动所有拉杆的期望奖励估值
        self.estimates = np.array([init_prob] * self.bandit.K)

    def run_one_step(self):
        if np.random.random() < self.epsilon:
            k = np.random.randint(0, self.bandit.K)  # 随机选择一根拉杆
        else:
            k = np.argmax(self.estimates)  # 选择期望奖励估值最大的拉杆
        r = self.bandit.step(k)  # 得到本次动作的奖励
        self.estimates[k] += 1. / (self.counts[k] + 1) * (r - self.estimates[k])
        return k
```
可以取ε和t反比例衰减等。

2. 上置信界算法
引入不确定性度量U(a)，记期望奖励估值Q^。随着动作尝试次数的增加而减小。上置信界算法利用霍夫丁不等式：若X~1~, ...X~n~取值范围为[0, 1]，经验期望x~n~ = 1/nΣX~j~，则P{E(X) >= x~n~ + u} <= exp(-2nu^2^)，将Q^~t~(a)代入x~t~，u = U~t~(a)，给定概率p = exp(-2N~t~(a)U~t~(a)^2^)，则Q~t~(a) < Q^~t~(a) + U~t~(a)至少以1-p概率成立，取期望奖励上界最大的动作，即a = argmax[Q^ + U]，其中U = sqrt(-logp/2N~t~(a))
实施时可以设置p = 1/t。

### 动态规划算法
假设有4x12的网格，智能体在左下角，目标在右下角，智能体能上下左右移动，每走一步奖励-1，掉入悬崖奖励-100。
#### 策略迭代算法
1. 策略评估
根据贝尔曼期望方程，V~Π~(s) = ΣΠ(a|s)(r(s,a) + γΣp(s'|s,a)V~Π~(s'))，可以根据下一个状态价值计算当前状态价值。考虑迭代V~k+1~(s) = ΣΠ(a|s)(r(s,a) + γΣp(s'|s,a)V~k~(s'))，选定任意初始值V~0~，上述更新的不动点是V~Π~。

2. 策略提升
假设对于策略Π，已知价值V~Π~，如果存在策略Π'，对于每个状态s，有Q~Π~(s,Π'(s)) >= V~Π~(s)，则任意状态下V~Π'~(s) >= V~Π~(s)。故可以选择Π' = argmaxQ~Π~(s,a) = argmax{r(s,a) + γΣP(s'|s,a)V~Π~(s')}

3. 算法实现
```
class PolicyIteration:  
    def __init__(self, env, theta, gamma):
        self.env = env
        self.v = [0] * self.env.ncol * self.env.nrow
        self.pi = [[0.25, 0.25, 0.25, 0.25] for i in range(self.env.ncol * self.env.nrow)]
        self.theta = theta # 策略评估收敛阈值
        self.gamma = gamma

    def policy_evaluation(self):
        cnt = 1
        while 1:
            max_diff = 0
            new_v = [0] * self.ncol * self.env.nrow
            for s in range(self.env.ncol * self.env.nrow):
                # 计算s状态下的所有Q(s,a)价值
                qsa_list = []
                for a in range(4):
                    qsa = 0
                    for res in self.env.P[s][a]:
                        p, next_state, r, done = res
                        qsa += p * (r + self.gamma * self.v[next_state])
                    qsa_list.append(self.pi[s][a] * qsa)
                new_v[s] = sum(qsa_list)
                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))
            self.v = new_v
            if max_diff < self.theta: break
            cnt += 1

    def policy_improvement(self):
        for s in range(self.env.nrow * self.env.ncol):
            qsa_list = []
            for a in range(4):
                qsa = 0
                for res in self.env.P[s][a]
                    p, next_state, r, done = res
                    qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))
                qsa_list.append(qsa)
            maxq = max(qsa_list)
            cntq = qsa_list.count(maxq)
            # 让动作均分概率
            self.pi[s] = [1/cntq if q == maxq else 0 for q in qsa_list]

    def policy_iteration(self):
        while 1:
            self.policy_evaluation()
            old_pi = copy.deepcopy(self.pi)
            new_pi = self.policy_improvement()
            if old_pi == new_pi: break
```

#### 价值迭代算法
利用贝尔曼最优房产V*(s) = max{r(s,a) + γΣP(s'|s,a)V*(s')}，得到迭代更新的方式，并返回确定性策略Π(s) = argmaxV*

## 进阶内容
### DQN详解
1. 经验回放：维护一个回放缓冲区，将每次从环境中采样得到的四元组数据（状态，动作，奖励，下个状态）存储到缓冲区，训练Q网络时再从缓冲区随机采样若干数据。
2. 目标网络：原来的训练网络用来计算原先损失函数L = 1/2[Q~w~(s,a) - (r + γmaxQ~w-~(s',a'))]^2^中的Q~w~项，每次都会更新。目标网络Q~w-~用于计算r + γmaxQ~w-~(s',a')项，每隔C步同步一次。

代码实现：
```
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)

    def add(self, state, action, reward, next_state, done):
        self.buffer.append(state, action, reward, next_state, done)

    def sample(self, batch_size):
        transitions = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*transitions)
        return np.array(state), action, reward, np.array(next_state)

    def size(self):
        return len(self.buffer)

class Qnet(nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(Qnet, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

class DQN:
    def __init__(self, state_dim, hidden_dim, action_dim, lr, gamma,
                 epsilon, target_update, device):
        self.action_dim = action_dim
        self.q_net = Qnet(state_dim, hidden_dim, self.action_dim.to(device))
        # 目标网络
        self. target_q_net = Qnet(state_dim, hidden_dim, self.action_dim.to(device))
        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)
        self.gamma = gamma
        self.epsilon = epsilon
        # 目标网络更新频率
        self.target_update = target_update
        self.count = 0
        self.device = device

    def take_action(self, state):
        if np.random.random() < self.epsilon:
            action = np.random.randint(self.action_dim)
        else:
            state = torch.tensor([state], dtype=torch.float).to(self.device)
            action = self.q_net(state).argmax().item()
        return action

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        # 从qnet输出中获取与action对应的q值
        q_values = self.q_net(states).gather(1, actions)
        # 下个状态的最大Q值
        max_next_q_values = self.target_q_net(next_states).max(1)[0]
        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)
        # 均方误差
        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))
        self.optimizer.zero_grad()
        dqn_loss.backward()
        self.optimizer.step()

        if self.count % self.target_update == 0:
            self.target_q_net.load_state_dict(self.q_net.state_dict())
        self.count += 1
```
其中获取Q值采用的是module中的gather函数，对gather的理解是：
```
假设index=[[x1, x2, x3],
           [y1, y2, y3],
           [z1, z2, z3]]
如果dim=0，列不变，取代行：
[[(x1,0), (x2,1), (x3,2)],
 [(y1,0), (y2,1), (y3,2)],
 [(z1,0), (z2,1), (z3,2)]]，如果dim=1，则取代列
```
module中的方法load_state_dict可以用来加载模型参数，可以将Qnet加入卷积层以训练图像输入。

### 改进DQN算法
#### Double DQN
传统DQN计算TD误差目标时先选取状态s'下的最优动作a*，再计算该动作对应的价值Q~w-~(s', a*)，当两部分采用同一套Q网络计算时，每次得到的都是神经网络估算的所有动作价值中的最大值，因此会产生正向误差累计。

Double DQN算法提出用两个独立训练的神经网络估算maxQ~w-~(s',a')，将其更改为Q~w-~(s', argmaxQ~w~(s', a'))，即用Q~w~的输出选取价值最大的动作，用另一套计算其价值。

#### Dueling DQN
将状态动作价值函数Q减去状态价值函数记为优势函数A，Q网络被建模为Q~η,α,β~(s,a) = V~η,α~(s) + A~η,β~(s,a)。η是状态价值函数和优势函数共享的网络参数。

对V值，A值建模不唯一性的问题，可以强制最优动作的优势函数输出为0，即：Q(s,a) = V(s) + A(s,a) - maxA(s,a')，此时V(s) = maxQ(s,a)，也可以选取平均：Q(s,a) = V(s) + A(s,a) - 1/A * ΣA(s,a')，此时V(s) = 1/A * ΣQ(s,a')

代码实现：定义状态价值函数和优势函数的复合神经网络VAnet
```
class VAnet(torch.nn.Module):
    ''' 只有一层隐藏层的A网络和V网络 '''
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(VAnet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)  # 共享网络部分
        self.fc_A = torch.nn.Linear(hidden_dim, action_dim)
        self.fc_V = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        A = self.fc_A(F.relu(self.fc1(x)))
        V = self.fc_V(F.relu(self.fc1(x)))
        Q = V + A - A.mean(1).view(-1, 1)  # Q值由V值和A值计算得到
        return Q
```
注意：q_net(state)得到的是形状为(batch_size, action_dim)的Q值，q_net(state).max(1)寻找动作维度的最大值，[1]获取动作索引。
在update中的更改：
```
if self.dqn_type == 'Dueling DQN': # DQN与Double DQN的区别
    max_action = self.q_net(next_states).max(1)[1].view(-1, 1)
    max_next_q_values = self.target_q_net(next_states).gather(1, max_action)
```

### REINFORCE算法
利用蒙特卡洛方法估计Q~Π~(s,a) = Σ~t'=t~γ^t'-t^r~t'~ = ζ~t~，对策略参数θ进行上升梯度更新θ = θ + αΣζ~t~d(lnΠ~θ~(a~t~|s~t~))/dt。
算法实践：
```
import gym
import torch.optim
from torch import nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import rl_utils
import torch

class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x))


class REINFORCE:
    def __init__(self, state_dim, hidden_dim, action_dim, lr,
                 gamma, device):
        self.policy_net = PolicyNet(state_dim, hidden_dim,
                                    action_dim).to(device)
        self.optimizer = torch.optim.Adam(self.policy_net.parameters(),
                                          lr=lr)
        self.gamma = gamma
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        # 策略网络获取动作的概率分布
        probs = self.policy_net(state)
        # 根据动作概率分布随机采样
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    def update(self, transition_dict):
        reward_list = transition_dict['rewards']
        state_list = transition_dict['states']
        action_list = transition_dict['actions']

        G = 0
        self.optimizer.zero_grad()
        # 从最后一步算起
        for i in reversed(range(len(reward_list))):
            reward = reward_list[i]
            state = torch.tensor([state_list[i]],
                                 dtype=torch.float).to(device)
            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)
            log_prob = torch.log(self.policy_net(state).gather(1, action))
            G = self.gamma * G + reward
            loss = -log_prob * G
            loss.backword()
        self.optimizer.step()
```
其中torch.distributions.Categorical函数根据传入的概率probs生成概率分布
注意：
1. 新版gym中将env.seed替换为env.reset(seed=0)
2. env.step返回五个参数observation(next_state)，reward，terminated，truncated，info，done为terminated or truncated。
3. 新版gym中对于discrete空间，没有了env.action_space.n，可以print(env.action_space)查看其维度
4. 新版中s = env.reset()改为env.reset()[0]

### Actor-Critic
REINFORECE算法中的ζ~t~有很多种形式，还可以是Σγ^t'-t^r~t'~ - b(s~t~)，其中b为基线函数，用于减小方差。还可以用动作价值函数Q~Πθ~，优势函数A~Πθ~或时序差分残差r~t~ + γV~Πθ~(s~t+1~) - V~Πθ~(s~t~).
对于Actor采用策略梯度原则，对Critic采用损失函数L(w) = 1/2(r + γV~w~(s~t+1~) - V~w~(s~t~))^2^，梯度为-(r + γV~w~(s~t+1~) - V~w~(s~t~))dV~w~(s~t~)/dt。
代码实现：
额外引入一个价值网络
```
class ValueNet(nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```
定义ActorCritic算法：
```
class ActorCritic:
    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,
                 gamma, device):
        # 策略网络
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        self.critic = ValueNet(state_dim, hidden_dim).to(device)  # 价值网络
        # 策略网络优化器
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),
                                                lr=actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),
                                                 lr=critic_lr)  # 价值网络优化器
        self.gamma = gamma
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.actor(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(
            self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)

        # 时序差分目标
        td_target = rewards + self.gamma * self.critic(next_states) * (1 -
                                                                       dones)
        td_delta = td_target - self.critic(states)  # 时序差分误差
        log_probs = torch.log(self.actor(states).gather(1, actions))
        actor_loss = torch.mean(-log_probs * td_delta.detach())
        # 均方误差损失函数
        critic_loss = torch.mean(
            F.mse_loss(self.critic(states), td_target.detach()))
        self.actor_optimizer.zero_grad()
        self.critic_optimizer.zero_grad()
        actor_loss.backward()  # 计算策略网络的梯度
        critic_loss.backward()  # 计算价值网络的梯度
        self.actor_optimizer.step()  # 更新策略网络的参数
        self.critic_optimizer.step()  # 更新价值网络的参数
```
其中td_target.detach()将张量从计算图中分离，因此计算梯度时不会影响到它。

### DDPG算法
REINFORCE和ActorCritic算法都是在线策略算法，样本效率低，DQN算法直接估计Q，可做到离线策略学习，但只能处理动作空间有限的环境。DDPG创造一个确定性策略，用梯度上升的方法最大化Q值
策略是固定的，记a = μ~θ~(s)，dJ(Π~θ~) = E~s~[d~θ~(μ~θ~(s))d~a~Q~w~(s,a)]。首先Q对μ~θ~求导，其中由链式法则得先对a求导再对θ求导。然后通过梯度上升最大化Q，得到Q值最大的动作。

DDPG需要四个神经网络，Actor和Critic用去一个，此外各需要一个目标网络。目标网络采取软更新方式，w^-^ = rw + (1-r)w^-^，r是一个小的数。
流程如下：
1. 用随机参数w和θ初始化Critic网络Q~w~和Actor网络μ~θ~，复制参数w^-^ = w, θ^-^ = θ，初始化目标网络
2. 初始化经验回放池R
3. 进入循环：
```
for e=1->E do:
    初始化随机过程N
    获取初始状态s~1~
    for 时间步t=1->T do:
        根据策略和噪声选择动作a~t~ = μ~θ~(s~t~) + N
        执行动作a~t~，获得奖励r~t~，环境变为s~t+1~
        将(s~t~, a~t~, r~t~, s~t+1~)存储进回放池R
        从R中采样N个元组
        对每个元组用目标网络计算y~i~ = r~i~ + γQ~w-~
        最小化目标损失L，更新Critic网络
        计算策略梯度以更新Actor网络
        更新目标网络
```
代码实现：
对于策略网络和价值网络，采取正切函数tanhx作为激活函数，方便按比例调整成环境可以接受的动作范围。Q网络输入的是状态和动作拼接后的向量，输出是一个值，表示状态动作对的价值
```
class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)
        self.action_bound = action_bound  # action_bound是环境可以接受的动作最大值

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return torch.tanh(self.fc2(x)) * self.action_bound

class QValueNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(QValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)
        self.fc_out = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x, a):
        cat = torch.cat([x, a], dim=1) # 拼接状态和动作
        x = F.relu(self.fc1(cat))
        x = F.relu(self.fc2(x))
        return self.fc_out(x)
```
主体部分，采用正态分布噪声
```
class DDPG:
    def __init__(self, state_dim, hidden_dim, action_bound, sigma,
                 actor_lr, critic_lr, tau, gamma, device):
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)
        self.critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)
        self.target_actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)
        self.target_critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)
        # 初始化目标网络
        self.target_critic.load_state_dict(self.critic.state_dict())
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)
        self.gamma = gamma
        self.sigma = sigma  # 高斯噪声的标准差,均值直接设为0
        self.tau = tau  # 目标网络软更新参数
        self.action_dim = action_dim
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        action = self.actor(state).item()
        # 给动作添加噪声，增加探索
        action = action + self.sigma * np.random.randn(self.action_dim)
        return action

    def soft_update(self, net, target_net):
        for param_target, param in zip(target_net.parameters(), net.parameters()):
            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)

        def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions'], dtype=torch.float).view(-1, 1).to(self.device)
        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)

        # 更新critic网络
        next_q_values = self.target_critic(next_states, self.target_actor(next_states))
        q_targets = rewards + self.gamma * next_q_values * (1 - dones)
        critic_loss = torch.mean(F.mse_loss(self.critic(states, actions), q_targets))
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # 更新actor网络
        actor_loss = -torch.mean(self.critic(states, self.actor(states)))
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        self.soft_update(self.actor, self.target_actor)
        self.soft_update(self.critic, self.target_critic)
```
其中利用target_param.data.copy_来给目标网络更新参数

### SAC算法
DDPG训练不稳定，收敛性差，对超参数很敏感，SAC属于最大熵强化学习。
#### 最大熵强化学习
X为随机变量，概率密度为p，则其熵H(X) = E[-logp(x)]。用H(Π(.|s))表示Π在s状态的随机程度。
最大熵强化学习除了最大化累计奖励，还要使策略更随机，因此加入熵的正则项，定义为Π* = argmaxE~Π~[Σr(s~t~, a~t~) + αH(Π)]，α用来控制熵的重要程度。

#### Soft策略迭代
Soft贝尔曼方程：Q(s~t~, a~t~) = r(s~t~, a~t~) + γE[V(s~t+1~)]
其中状态价值函数V(s~t~)改写为V(s~t~) = E~at~[Q(s~t~, a~t~)] + H(Π)。
根据Soft策略提升公式：Π~new~ = argminD~KL~(Π', exp(1/αQ~Πold~(s,))/Z~Πold~(s,))改进策略。

#### SAC
为两个动作价值函数Q和一个策略函数Π建模，基于DQN思想，SAC使用两个Q网络，但每次挑选一个Q值小的网络
L~Q~(w) = E[1/2(Q~w~(s~t~,a~t~) - (r~t~ + γV~w-~(s~t+1~)))^2^] = E[1/2(Q~w~ - (r~t~ + γ(minQ~wj-~(s~t+1~,a~t+1~) - alogΠ)))^2^]
策略Π的损失函数由KL散度得到，化简后为L~Π~(θ) = E[αlog(Π~θ~(a~t~|s~t~)) - Q~w~(s~t~,a~t~)]，可以理解为最大化函数V。

对连续动作空间，需要重参数化技巧，先从单位高斯分布N中采样，再把采样值乘以标准差后加上均值，表示为a~t~ = f~θ~(ε;s~t~)，则策略的损失函数为：L~Π~(θ) = E[αlog(Π~θ~(f~θ~|s~t~)) - minQ~wj~(s~t~,f~θ~)]
在不同状态下需要不同大小的熵：最优动作不确定时，熵的取值应大一点。为自动调整正则项，有一带约束的优化问题：maxE[Σr(s~t~,a~t~)] s.t. E[-log(Π~t~(a~t~|s~t~))] >= H~0~，得到α的损失函数L(α) = E[-αlogΠ - αH~0~]

代码实现：
策略网络输出一个高斯分布的均值和标准差表示动作分布
```
class PolicyNetContinuous(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):
        super(PolicyNetContinuous, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)
        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)
        self.action_bound = action_bound

    def forward(self, x):
        x = F.relu(self.fc1(x))
        mu = self.fc_mu(x)
        std = F.softplus(self.fc_std(x))
        dist = Normal(mu, std)
        normal_sample = dist.rsample()  # rsample()是重参数化采样
        log_prob = dist.log_prob(normal_sample)
        action = torch.tanh(normal_sample)
        # 计算tanh_normal分布的对数概率密度
        log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)
        action = action * self.action_bound
        return action, log_prob


class QValueNetContinuous(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(QValueNetContinuous, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)
        self.fc_out = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x, a):
        cat = torch.cat([x, a], dim=1)
        x = F.relu(self.fc1(cat))
        x = F.relu(self.fc2(x))
        return self.fc_out(x)
```
策略网络中有dist.rsample()进行重采样，再取对数得到log(Π~θ~(f~θ~|s~t~))，对log_prob的操作得到tanh之后动作的对数概率密度。

SAC使用两个Critic网络使Actor训练稳定，各自需要一个目标价值网络。
```
class SACContinuous:
    def __init__(self, state_dim, hidden_dim, action_dim, action_bound,
                 actor_lr, critic_lr, alpha_lr, target_entropy, tau, gamma,
                 device):
        self.actor = PolicyNetContinuous(state_dim, hidden_dim, action_dim,
                                         action_bound).to(device)  # 策略网络
        self.critic_1 = QValueNetContinuous(state_dim, hidden_dim,
                                            action_dim).to(device)  # 第一个Q网络
        self.critic_2 = QValueNetContinuous(state_dim, hidden_dim,
                                            action_dim).to(device)  # 第二个Q网络
        self.target_critic_1 = QValueNetContinuous(state_dim,
                                                   hidden_dim, action_dim).to(
                                                       device)  # 第一个目标Q网络
        self.target_critic_2 = QValueNetContinuous(state_dim,
                                                   hidden_dim, action_dim).to(
                                                       device)  # 第二个目标Q网络
        # 令目标Q网络的初始参数和Q网络一样
        self.target_critic_1.load_state_dict(self.critic_1.state_dict())
        self.target_critic_2.load_state_dict(self.critic_2.state_dict())
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),
                                                lr=actor_lr)
        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),
                                                   lr=critic_lr)
        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),
                                                   lr=critic_lr)
        # 使用alpha的log值,可以使训练结果比较稳定
        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)
        self.log_alpha.requires_grad = True  # 可以对alpha求梯度
        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],
                                                    lr=alpha_lr)
        self.target_entropy = target_entropy  # 目标熵的大小
        self.gamma = gamma
        self.tau = tau
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        action = self.actor(state)[0]
        return [action.item()]

    # 计算y
    def calc_target(self, rewards, next_states, dones):
        next_actions, log_prob = self.actor(next_states)
        entropy = -log_prob
        q1_value = self.target_critic_1(next_states, next_actions)
        q2_value = self.target_critic_2(next_states, next_actions)
        next_value = torch.min(q1_value, q2_value) + self.log_alpha.exp() * entropy
        td_target = rewards + self.gamma * next_value * (1 - done)

    # 软更新
    def soft_update
        for param_target, param in zip(target_net.parameters(), net,parameters())
        param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        # 和之前章节一样,对倒立摆环境的奖励进行重塑以便训练
        rewards = (rewards + 8.0) / 8.0

        # 更新两个Q网络
        td_target = self.calc_target(rewards, next_states, dones)
        critic_1_loss = torch.mean(F.mse_loss(self.critic_1(states, actions), td_target.detach()))
        critic_2_loss = torch.mean(F.mse_loss(self.critic_2(states, actions), td_target.detach()))
        self.critic_1_optimizer.zero_grad()
        critic_1_loss.backward()
        self.critic_1_optimizer.step()
        self.critic_2_optimizer.zero_grad()
        critic_2_loss.backward()
        self.critic_2_optimizer.step()

        # 更新策略网络
        new_actions, log_prob = self.actor(states)
        entropy = -log_prob
        q1_value = self.critic_1(states, new_actions)
        q2_value = self.critic_2(states, new_actions)
        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - torch.min(q1_value))
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # 更新alpha值
        alpha_loss = torch.mean(entropy - self.target_entropy).detach() * self.log_alpha.exp()
        self.log_alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.log_alpha_optimizer.step()

        self.soft_update(self.critic_1, self.target_critic_1)
        self.soft_update(self.critic_2, self.target_critic_2)
```